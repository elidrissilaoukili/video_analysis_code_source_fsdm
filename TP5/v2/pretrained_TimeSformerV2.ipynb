{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mohammed elidrissi laoukili\n",
    "* subjet  : video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T11:26:05.008245Z",
     "iopub.status.busy": "2026-01-02T11:26:05.007974Z",
     "iopub.status.idle": "2026-01-02T11:29:08.054161Z",
     "shell.execute_reply": "2026-01-02T11:29:08.053306Z",
     "shell.execute_reply.started": "2026-01-02T11:26:05.008221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 11:26:21.102929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767353181.324237      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767353181.387863      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIMESFORMER VIDEO EMOTION RECOGNITION\n",
      "Using Hugging Face Transformers + Facebook Research Pretrained Model\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  num_frames: 8\n",
      "  frame_size: 224\n",
      "  batch_size: 2\n",
      "  num_epochs: 5\n",
      "  learning_rate: 3e-05\n",
      "  weight_decay: 0.01\n",
      "  device: cuda\n",
      "  use_pretrained: True\n",
      "  train_split: 0.6\n",
      "  val_split: 0.2\n",
      "  test_split: 0.2\n",
      "================================================================================\n",
      "\n",
      "üìÅ Loading dataset...\n",
      "Loading dataset (split: train)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d097a018594946b4338d924bee3684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b205922c1c6f4f9ea922ddcf1d532ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cea0f7452114143a8cf523e16176f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/101 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c10968720eb4d988a6691d373536dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/006_20250111_1454_Joyful Laughter (‚Ä¶):   0%|          | 0.00/3.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36edb844a07945a5b18c253c561e53c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/003_20250111_1450_Joyful Leap_simp(‚Ä¶):   0%|          | 0.00/4.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18be33379a7c46b4a1017d5364cadfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/005_20250111_1457_Joyful Laughter (‚Ä¶):   0%|          | 0.00/4.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1146d8e76d4b423b9b20dd8595184d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/010_20250111_1459_Joyful Balloon R(‚Ä¶):   0%|          | 0.00/3.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ea674cef6d43e494bde97cbdbc672c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/004_20250111_1450_Warm Wisdom Smil(‚Ä¶):   0%|          | 0.00/3.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94498ec45494034952c276a8a718fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/015_20250111_1503_Matriarch's Ster(‚Ä¶):   0%|          | 0.00/4.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3184969182494c048e386c3182d05738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/011_20250111_1500_Fired Up!_simple(‚Ä¶):   0%|          | 0.00/3.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474fbbb89e274e1ba2ca42dc20069d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/008_20250111_1456_Joyful Dance Mov(‚Ä¶):   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7027ee6aa12544bfb6f4e9adcd5d9231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/013_20250111_1502_Intense Confront(‚Ä¶):   0%|          | 0.00/3.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6660a10b4f4ebb8a302ee94b705844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/001_20250107_2153_Joyful Laughter_(‚Ä¶):   0%|          | 0.00/3.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf1705820ad4f8a98f56f52e13c22f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/016_20250111_1504_Intense Frustrat(‚Ä¶):   0%|          | 0.00/2.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96f18f396b3449084071756fd613faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/007_20250111_1455_Father's Joyful (‚Ä¶):   0%|          | 0.00/3.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dfe2c9184bf46cdac3ea8e3ab093a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/002_20250111_1449_Joyful Celebrati(‚Ä¶):   0%|          | 0.00/6.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b67cbc06944949b55916ac5abe5bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/012_20250111_1500_Frustrated Refle(‚Ä¶):   0%|          | 0.00/2.58M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf2dd514c804c01a479165d3fe2be49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/014_20250111_1503_Angry Boy's Stom(‚Ä¶):   0%|          | 0.00/4.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e946a5814c464b58b54c541f69615321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/009_20250111_1458_Grandfather's Jo(‚Ä¶):   0%|          | 0.00/3.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18cb851034554abebff2c7e2e723f239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/017_20250111_1506_Passionate Kitch(‚Ä¶):   0%|          | 0.00/2.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03853f94461c4c6394ec0f3e6427ec99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/018_20250111_1506_Frustrated Homew(‚Ä¶):   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ac33fdaea94ed59d5d81d7fa8d33b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/019_20250111_1519_Desert Boy's Sho(‚Ä¶):   0%|          | 0.00/3.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed53c99e1e744c9a6b4de8bea2ee2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/020_20250111_1520_Intense Confront(‚Ä¶):   0%|          | 0.00/2.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53f50758b114290bba54912bf7d180f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/021_20250119_0051_Lonely Tears_sim(‚Ä¶):   0%|          | 0.00/2.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa22db4821c41acb5eaec2da1982028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/022_20250111_1522_Reflective Momen(‚Ä¶):   0%|          | 0.00/4.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01021df9b5a9476c8ad15453873d4353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/024_20250111_1523_Heartfelt Solitu(‚Ä¶):   0%|          | 0.00/2.77M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac7a86d67f5436eb71feb187a643e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/023_20250111_1523_Lonely Park Thou(‚Ä¶):   0%|          | 0.00/2.45M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924ef787fa304fcf880754158bbf1f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/025_20250111_1525_Tearful Solitude(‚Ä¶):   0%|          | 0.00/2.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022ac67362444eabb2591df29d3de89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/026_20250111_1525_Raw Emotional Te(‚Ä¶):   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47dbc29bfb2c493eabba04faa2d8ca99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/027_20250111_1526_Mother's Comfort(‚Ä¶):   0%|          | 0.00/2.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c42906a1e4047918a017f20a41dc0fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/029_20250111_1531_Desolate Gaze_si(‚Ä¶):   0%|          | 0.00/2.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8572981750a4af3a26b73240b308811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/028_20250111_1527_Melancholic Cont(‚Ä¶):   0%|          | 0.00/3.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3057274b0a134ebf9c6ee94ebb0551c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/030_20250111_1528_Thai Girl's Tear(‚Ä¶):   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91321bf21344446b9517d4e51b8cf006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/031_20250111_1529_Fearful Retreat_(‚Ä¶):   0%|          | 0.00/3.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89b06fb1d194e0da542ba3e43f79dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/032_20250111_1531_Surprised Wonder(‚Ä¶):   0%|          | 0.00/2.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf6ac31b3334fb98abc7ba12222afe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/033_20250111_1534_Icelandic Fearfu(‚Ä¶):   0%|          | 0.00/3.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5d9e5d2d5b4ee08698d5c675652efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/034_20250111_1532_Hiding in Fear_s(‚Ä¶):   0%|          | 0.00/3.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7811069455be426099b2b179b08aaf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/037_20250111_1540_Desperate Scream(‚Ä¶):   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8434b9acbf5f46f49f1e614b6d7ccbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/036_20250111_1539_Defensive Tensio(‚Ä¶):   0%|          | 0.00/3.62M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a53214dc62d4a019a61ffa27ef53b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/035_20250111_1538_Trembling in Ten(‚Ä¶):   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086be186c73a458592d90daf5b861764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/038_20250111_1540_Teenager's Tense(‚Ä¶):   0%|          | 0.00/3.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78283dff777f4ecba1fbfe7fc1b2761c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/039_20250111_1541_Moment of Terror(‚Ä¶):   0%|          | 0.00/2.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d254bbe44e4c1b94bb153f833eda92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/040_20250111_1542_Protective Embra(‚Ä¶):   0%|          | 0.00/3.55M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c8b9425a47436e893914ce7d671d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/041_20250111_1930_Joyful Gift Disc(‚Ä¶):   0%|          | 0.00/3.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18061381c02a460686b701e8499d5319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/042_20250111_1931_Irish Astonishme(‚Ä¶):   0%|          | 0.00/3.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2af8653c2b44953a892114e95fd7580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/043_20250111_1933_Unexpected Surpr(‚Ä¶):   0%|          | 0.00/4.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399607b4353d4798aec255a87105d473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/044_20250111_1934_Joyful Surprise_(‚Ä¶):   0%|          | 0.00/3.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da5e833f8af4478b98b246030d7c7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/045_20250112_1548_Surprised Discov(‚Ä¶):   0%|          | 0.00/3.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe8f78eb3ae4e28aa0adcb6945e1c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/046_20250112_1549_Joyous Kite Adve(‚Ä¶):   0%|          | 0.00/4.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "127b9067db29414187dd2b2c2eed33b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/047_20250112_1551_Joyful Surprise (‚Ä¶):   0%|          | 0.00/4.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60bd887867f4c47aacaa490ae4f0553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/048_20250112_1552_Unexpected Book (‚Ä¶):   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aeb83c7c164211b9b06a76bded6b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/050_20250112_1556_Sudden Realizati(‚Ä¶):   0%|          | 0.00/2.44M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8253f2d0014344c0a984d6cee11f2105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/051_20250112_1608_Teen's Disgusted(‚Ä¶):   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6533a39e25641b6ac1f0067e66c86bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/052_20250112_1610_Expressive Displ(‚Ä¶):   0%|          | 0.00/4.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f8e3d7761e4dbfab774c5ec1a07d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/049_20250112_1555_Shocked City Gir(‚Ä¶):   0%|          | 0.00/4.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2d70b7d3fb499ebeab565fdebd9108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/053_20250112_1612_Unpleasant Smell(‚Ä¶):   0%|          | 0.00/3.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d175bbbe2904f70a9c393c38dceea9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/055_20250112_1644_Boy Rejects Curr(‚Ä¶):   0%|          | 0.00/2.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f5bfd85085493dbb511f45ce58464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/054_20250112_1621_Latina's Yuck Fa(‚Ä¶):   0%|          | 0.00/4.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1f36b48d784dca811837aa6d73763b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/056_20250112_1646_Pensive Middle E(‚Ä¶):   0%|          | 0.00/2.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be63fe9925c44091930d301d7c35023f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/057_20250112_1651_Stomach Pain Str(‚Ä¶):   0%|          | 0.00/3.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4747a41a6a34c6f9acfa508bb9043e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/058_20250112_1654_Unexpected Kitch(‚Ä¶):   0%|          | 0.00/4.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d726c41e2db45dfb7ddf101d08c041b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/059_20250112_1656_Vietnamese Marke(‚Ä¶):   0%|          | 0.00/3.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef017913832410da04ab8162686fa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/062_20250112_1704_Mother's Warm Em(‚Ä¶):   0%|          | 0.00/3.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c2d00d394a44138805487bc72153c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/060_20250112_1657_Grandfather's Ge(‚Ä¶):   0%|          | 0.00/2.81M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5367e8038d274e8ab3e8361bce463f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/061_20250112_1701_Joyful Park Mome(‚Ä¶):   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd902defb5d4fe2b86100165d7322f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/063_20250112_1708_Shy Teen's Confi(‚Ä¶):   0%|          | 0.00/3.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a3db72b38a45fd8a13c57c74915d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/064_20250112_1709_Warm Grandmother(‚Ä¶):   0%|          | 0.00/2.97M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f46ae654669418cba3c9decc39b3f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/065_20250112_1714_Warm Embrace_sim(‚Ä¶):   0%|          | 0.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19663e6d249b43cca695ad9c02e9ee5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/066_20250112_1724_Tender Father-Ba(‚Ä¶):   0%|          | 0.00/2.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b753e8412eb49a5a22321182762c231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/067_20250112_1725_Youthful Hope in(‚Ä¶):   0%|          | 0.00/3.49M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d1508bd1e40dab18e6a2987f1f394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/068_20250112_1727_Cozy Feline Comp(‚Ä¶):   0%|          | 0.00/4.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605da18c792c40529273fb769955fa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/069_20250112_1729_Joyful Heart Ges(‚Ä¶):   0%|          | 0.00/3.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d09c519f3243aaa9a347c4b5f586c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/072_20250112_1736_Deep in Thought_(‚Ä¶):   0%|          | 0.00/2.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cd62b5a7644306ade524f667191f82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/070_20250112_1730_Tender African C(‚Ä¶):   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f137e52b212c42a39b82ba60b1d38493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/071_20250112_1732_Perplexed in Sun(‚Ä¶):   0%|          | 0.00/2.54M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ec312d5344494984d1521e5508dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/073_20250112_1739_Elderly Woman's (‚Ä¶):   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fa9cede0af4cfb95f6dc894c63db2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/074_20250112_1740_Confused Shrug E(‚Ä¶):   0%|          | 0.00/2.47M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc70ff2bc62451489b1e6a9eece566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/075_20250112_1742_Lost in Tokyo_si(‚Ä¶):   0%|          | 0.00/4.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac4bb1dbca44b24a8c52f7bb77a37fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/076_20250112_1749_Pointing with Cu(‚Ä¶):   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6503a702e034abd9848d9c981cee669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/077_20250112_1750_Contemplative Re(‚Ä¶):   0%|          | 0.00/2.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a9746b27194d908961a18dc45c21e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/078_20250112_1802_Perplexed Mother(‚Ä¶):   0%|          | 0.00/2.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f4fd8f6d6242d5a3c8a68349ef581f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/079_20250112_1805_Teen's Upside-Do(‚Ä¶):   0%|          | 0.00/3.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9598e19b2284f508c78b5e8e9832c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/082_20250113_2340_Contemplative Mo(‚Ä¶):   0%|          | 0.00/3.18M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da7b2f16033437a9399468c2d7fdc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/080_20250113_2338_Confused Man's E(‚Ä¶):   0%|          | 0.00/2.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0da368ed93a40adb091088cbe0c881c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/081_20250113_2339_Tranquil Tea Mom(‚Ä¶):   0%|          | 0.00/2.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75f7d87fded4122b06d5ab66f72e1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/083_20250113_2342_Confident Young (‚Ä¶):   0%|          | 0.00/1.51M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dc1d5eae784b1b85efc28a23abd392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/084_20250113_2344_Late-Night Study(‚Ä¶):   0%|          | 0.00/2.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e7cd03d0724162a88039f590ce558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/086_20250113_2348_Peaceful Contemp(‚Ä¶):   0%|          | 0.00/2.88M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1664bce941e404ab41284e89c13da2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/085_20250113_2344_City Stroll Sere(‚Ä¶):   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0619e0a5bb514636841cf58aa3c6726a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/087_20250113_2349_Reflective Momen(‚Ä¶):   0%|          | 0.00/2.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60234a53eb7a4f61a0c08f35c1e7e63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/088_20250113_2350_Serene Shoelace (‚Ä¶):   0%|          | 0.00/3.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c686769827f4a378e44bc52da00edd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/089_20250113_2352_Impatient Market(‚Ä¶):   0%|          | 0.00/4.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578710d5b6574674b4f556db0bac38f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/090_20250113_2352_Hispanic Mother (‚Ä¶):   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba87e7d526c4cf5ba29d103e8eec096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/091_20250113_2353_Tears of Joy_sim(‚Ä¶):   0%|          | 0.00/3.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eab0feeddb4c74a3cf4733f07ff3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/093_20250113_2355_Endearing Nervou(‚Ä¶):   0%|          | 0.00/2.60M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef5e3170dfb4b8998fcb28eead422e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/094_20250113_2358_Emotional Releas(‚Ä¶):   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8344ccac4da44d47ab6a4277ffb5207d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/095_20250113_2359_Surprised to Joy(‚Ä¶):   0%|          | 0.00/4.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e358ffd808d64df4942f79b0bc90d4b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/092_20250113_2354_Frustration in S(‚Ä¶):   0%|          | 0.00/3.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4fa6ac18144f8ea725bae1713199fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/096_20250114_0002_Playful Sass Unl(‚Ä¶):   0%|          | 0.00/2.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632c7578709747ce977581500bb54dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/097_20250114_0003_Joyful Laughter (‚Ä¶):   0%|          | 0.00/3.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0022f4da9146dcaa3e9d8a5ea43621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/098_20250114_0004_Confused Amuseme(‚Ä¶):   0%|          | 0.00/2.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e146bc6f52eb4d1895d350303a10103f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/099_20250114_0005_From Frown to Sm(‚Ä¶):   0%|          | 0.00/4.53M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9afcc7d64274af193ece75c090665da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train/100_20250114_0008_Joyful Reconcili(‚Ä¶):   0%|          | 0.00/2.88M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1634d755e1c4c388408ab7fbe0d84cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "metadata.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea21df0d00940f1b49ff7db38b871cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 videos\n",
      "Emotions: ['Anger', 'Confusion', 'Disgust', 'Fear', 'Happiness and Joy', 'Love and Affection', 'Mixed Emotions', 'Neutral/Everyday', 'Sadness', 'Surprise']\n",
      "Distribution: {'Happiness and Joy': 10, 'Anger': 10, 'Sadness': 10, 'Fear': 10, 'Surprise': 10, 'Disgust': 10, 'Love and Affection': 10, 'Confusion': 10, 'Neutral/Everyday': 10, 'Mixed Emotions': 10}\n",
      "Loading dataset (split: train)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11019c02439d484a91a81cdcaba635b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 videos\n",
      "Emotions: ['Anger', 'Confusion', 'Disgust', 'Fear', 'Happiness and Joy', 'Love and Affection', 'Mixed Emotions', 'Neutral/Everyday', 'Sadness', 'Surprise']\n",
      "Distribution: {'Happiness and Joy': 10, 'Anger': 10, 'Sadness': 10, 'Fear': 10, 'Surprise': 10, 'Disgust': 10, 'Love and Affection': 10, 'Confusion': 10, 'Neutral/Everyday': 10, 'Mixed Emotions': 10}\n",
      "\n",
      "üìä Splits: Train=60, Val=20, Test=20\n",
      "\n",
      "üèóÔ∏è  Building TimeSformer model...\n",
      "   Loading pretrained TimeSformer from Facebook Research...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee2661012504f3398a0f2117ad78b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c76b7b9fac4d1f85294a5bf917b185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Pretrained weights loaded from Kinetics-400\n",
      "   Architecture:\n",
      "   - Attention Type: Divided Space-Time\n",
      "   - Hidden Size: 768\n",
      "   - Transformer Layers: 12\n",
      "   - Attention Heads: 12\n",
      "   - Output Classes: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0687ca0967422f9312bcbd134d82e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Parameters: 121.56M total, 121.56M trainable\n",
      "\n",
      "================================================================================\n",
      "üöÄ STARTING TRAINING\n",
      "================================================================================\n",
      "\n",
      "üìÖ Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "Training:   0%|          | 0/30 [00:02<?, ?it/s, loss=2.1303, acc=50.00%]\u001b[A\n",
      "Training:   3%|‚ñé         | 1/30 [00:02<01:04,  2.22s/it, loss=2.1303, acc=50.00%]\u001b[A\n",
      "Training:   3%|‚ñé         | 1/30 [00:02<01:04,  2.22s/it, loss=2.4167, acc=25.00%]\u001b[A\n",
      "Training:   7%|‚ñã         | 2/30 [00:02<00:37,  1.35s/it, loss=2.4167, acc=25.00%]\u001b[A\n",
      "Training:   7%|‚ñã         | 2/30 [00:03<00:37,  1.35s/it, loss=2.5039, acc=16.67%]\u001b[A\n",
      "Training:  10%|‚ñà         | 3/30 [00:03<00:28,  1.07s/it, loss=2.5039, acc=16.67%]\u001b[A\n",
      "Training:  10%|‚ñà         | 3/30 [00:04<00:28,  1.07s/it, loss=2.3094, acc=12.50%]\u001b[A\n",
      "Training:  13%|‚ñà‚ñé        | 4/30 [00:04<00:24,  1.07it/s, loss=2.3094, acc=12.50%]\u001b[A\n",
      "Training:  13%|‚ñà‚ñé        | 4/30 [00:05<00:24,  1.07it/s, loss=1.8268, acc=30.00%]\u001b[A\n",
      "Training:  17%|‚ñà‚ñã        | 5/30 [00:05<00:21,  1.16it/s, loss=1.8268, acc=30.00%]\u001b[A\n",
      "Training:  17%|‚ñà‚ñã        | 5/30 [00:05<00:21,  1.16it/s, loss=2.7146, acc=25.00%]\u001b[A\n",
      "Training:  20%|‚ñà‚ñà        | 6/30 [00:05<00:19,  1.22it/s, loss=2.7146, acc=25.00%]\u001b[A\n",
      "Training:  20%|‚ñà‚ñà        | 6/30 [00:06<00:19,  1.22it/s, loss=2.4425, acc=21.43%]\u001b[A\n",
      "Training:  23%|‚ñà‚ñà‚ñé       | 7/30 [00:06<00:18,  1.26it/s, loss=2.4425, acc=21.43%]\u001b[A\n",
      "Training:  23%|‚ñà‚ñà‚ñé       | 7/30 [00:07<00:18,  1.26it/s, loss=2.4355, acc=18.75%]\u001b[A\n",
      "Training:  27%|‚ñà‚ñà‚ñã       | 8/30 [00:07<00:17,  1.29it/s, loss=2.4355, acc=18.75%]\u001b[A\n",
      "Training:  27%|‚ñà‚ñà‚ñã       | 8/30 [00:08<00:17,  1.29it/s, loss=2.6421, acc=16.67%]\u001b[A\n",
      "Training:  30%|‚ñà‚ñà‚ñà       | 9/30 [00:08<00:16,  1.31it/s, loss=2.6421, acc=16.67%]\u001b[A\n",
      "Training:  30%|‚ñà‚ñà‚ñà       | 9/30 [00:08<00:16,  1.31it/s, loss=2.3648, acc=15.00%]\u001b[A\n",
      "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:08<00:15,  1.33it/s, loss=2.3648, acc=15.00%]\u001b[A\n",
      "Training:  33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:09<00:15,  1.33it/s, loss=2.4425, acc=13.64%]\u001b[A\n",
      "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:09<00:14,  1.34it/s, loss=2.4425, acc=13.64%]\u001b[A\n",
      "Training:  37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:10<00:14,  1.34it/s, loss=2.5363, acc=12.50%]\u001b[A\n",
      "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:10<00:13,  1.35it/s, loss=2.5363, acc=12.50%]\u001b[A\n",
      "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:11<00:13,  1.35it/s, loss=2.4058, acc=11.54%]\u001b[A\n",
      "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:11<00:12,  1.36it/s, loss=2.4058, acc=11.54%]\u001b[A\n",
      "Training:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:11<00:12,  1.36it/s, loss=2.2964, acc=10.71%]\u001b[A\n",
      "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:11<00:11,  1.36it/s, loss=2.2964, acc=10.71%]\u001b[A\n",
      "Training:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:12<00:11,  1.36it/s, loss=2.5414, acc=10.00%]\u001b[A\n",
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:12<00:10,  1.36it/s, loss=2.5414, acc=10.00%]\u001b[A\n",
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:13<00:10,  1.36it/s, loss=1.9171, acc=12.50%]\u001b[A\n",
      "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:13<00:10,  1.37it/s, loss=1.9171, acc=12.50%]\u001b[A\n",
      "Training:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:13<00:10,  1.37it/s, loss=2.2226, acc=11.76%]\u001b[A\n",
      "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:13<00:09,  1.36it/s, loss=2.2226, acc=11.76%]\u001b[A\n",
      "Training:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:14<00:09,  1.36it/s, loss=2.0633, acc=11.11%]\u001b[A\n",
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:14<00:08,  1.36it/s, loss=2.0633, acc=11.11%]\u001b[A\n",
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:15<00:08,  1.36it/s, loss=2.5474, acc=10.53%]\u001b[A\n",
      "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:15<00:08,  1.36it/s, loss=2.5474, acc=10.53%]\u001b[A\n",
      "Training:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:16<00:08,  1.36it/s, loss=2.4889, acc=10.00%]\u001b[A\n",
      "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:16<00:07,  1.36it/s, loss=2.4889, acc=10.00%]\u001b[A\n",
      "Training:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:16<00:07,  1.36it/s, loss=2.2999, acc=9.52%] \u001b[A\n",
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:16<00:06,  1.36it/s, loss=2.2999, acc=9.52%]\u001b[A\n",
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:17<00:06,  1.36it/s, loss=2.4968, acc=9.09%]\u001b[A\n",
      "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [00:17<00:05,  1.36it/s, loss=2.4968, acc=9.09%]\u001b[A\n",
      "Training:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [00:18<00:05,  1.36it/s, loss=2.1509, acc=10.87%]\u001b[A\n",
      "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [00:18<00:05,  1.37it/s, loss=2.1509, acc=10.87%]\u001b[A\n",
      "Training:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [00:19<00:05,  1.37it/s, loss=2.1567, acc=12.50%]\u001b[A\n",
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:19<00:04,  1.37it/s, loss=2.1567, acc=12.50%]\u001b[A\n",
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:19<00:04,  1.37it/s, loss=2.6046, acc=12.00%]\u001b[A\n",
      "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [00:19<00:03,  1.37it/s, loss=2.6046, acc=12.00%]\u001b[A\n",
      "Training:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [00:20<00:03,  1.37it/s, loss=2.4126, acc=11.54%]\u001b[A\n",
      "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:20<00:02,  1.37it/s, loss=2.4126, acc=11.54%]\u001b[A\n",
      "Training:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:21<00:02,  1.37it/s, loss=2.0400, acc=11.11%]\u001b[A\n",
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:21<00:02,  1.36it/s, loss=2.0400, acc=11.11%]\u001b[A\n",
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:22<00:02,  1.36it/s, loss=2.2447, acc=10.71%]\u001b[A\n",
      "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [00:22<00:01,  1.36it/s, loss=2.2447, acc=10.71%]\u001b[A\n",
      "Training:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [00:22<00:01,  1.36it/s, loss=2.2700, acc=10.34%]\u001b[A\n",
      "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:22<00:00,  1.36it/s, loss=2.2700, acc=10.34%]\u001b[A\n",
      "Training:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:23<00:00,  1.36it/s, loss=2.3258, acc=10.00%]\u001b[A\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:23<00:00,  1.28it/s, loss=2.3258, acc=10.00%]\u001b[A\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss=2.3417, Acc=0.1000\n",
      "Val:   Loss=2.6034, Acc=0.0000, F1=0.0000\n",
      "\n",
      "üìÖ Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:22<00:00,  1.35it/s, loss=2.2493, acc=10.00%]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss=2.3132, Acc=0.1000\n",
      "Val:   Loss=2.6096, Acc=0.0000, F1=0.0000\n",
      "\n",
      "üìÖ Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:22<00:00,  1.32it/s, loss=2.2749, acc=6.67%] \n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss=2.3505, Acc=0.0667\n",
      "Val:   Loss=2.5072, Acc=0.0500, F1=0.0048\n",
      "‚úÖ New best model saved!\n",
      "\n",
      "üìÖ Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:22<00:00,  1.31it/s, loss=2.3169, acc=6.67%]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss=2.3334, Acc=0.0667\n",
      "Val:   Loss=2.5483, Acc=0.0500, F1=0.0048\n",
      "\n",
      "üìÖ Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:23<00:00,  1.28it/s, loss=2.1693, acc=15.00%]\n",
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Loss=2.3106, Acc=0.1500\n",
      "Val:   Loss=2.5442, Acc=0.0500, F1=0.0048\n",
      "\n",
      "================================================================================\n",
      "üéØ FINAL TEST EVALUATION\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test Results:\n",
      "  Accuracy: 0.0500 (5.00%)\n",
      "  F1-Score: 0.0048\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TRAINING COMPLETE!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\"\"\"\n",
    "TimeSformer for Video Emotion Recognition using Hugging Face Transformers\n",
    "This approach uses pretrained TimeSformer from Facebook Research via HuggingFace\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from datasets import load_dataset\n",
    "import tempfile\n",
    "from transformers import TimesformerModel, TimesformerConfig\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 1. DATASET CLASS\n",
    "# ========================\n",
    "\n",
    "class SyntheticEmotionsDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for aadityaubhat/synthetic-emotions\"\"\"\n",
    "    def __init__(self, split='train', num_frames=8, frame_size=224, transform=None):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(f\"Loading dataset (split: {split})...\")\n",
    "        dataset = load_dataset(\"aadityaubhat/synthetic-emotions\", split=split)\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        self.samples = []\n",
    "        self.emotion_counts = {}\n",
    "        \n",
    "        labels = dataset['label'] if 'label' in dataset.column_names else dataset['emotion']\n",
    "        \n",
    "        for idx, emotion in enumerate(labels):\n",
    "            self.samples.append((idx, emotion))\n",
    "            self.emotion_counts[emotion] = self.emotion_counts.get(emotion, 0) + 1\n",
    "        \n",
    "        self.emotions = sorted(list(set([s[1] for s in self.samples])))\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.emotions)}\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} videos\")\n",
    "        print(f\"Emotions: {self.emotions}\")\n",
    "        print(f\"Distribution: {self.emotion_counts}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, emotion = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            item = self.dataset[sample_idx]\n",
    "            video_data = item['video']\n",
    "            frames = self.load_video(video_data)\n",
    "        except Exception as e:\n",
    "            frames = torch.zeros(self.num_frames, 3, self.frame_size, self.frame_size)\n",
    "        \n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        label = self.emotion_to_idx[emotion]\n",
    "        return frames, label\n",
    "    \n",
    "    def load_video(self, video_data):\n",
    "        \"\"\"Load and process video\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n",
    "            if isinstance(video_data, dict) and 'bytes' in video_data:\n",
    "                tmp.write(video_data['bytes'])\n",
    "            elif isinstance(video_data, bytes):\n",
    "                tmp.write(video_data)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        frames = self.extract_frames(tmp_path)\n",
    "        \n",
    "        try:\n",
    "            os.unlink(tmp_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def extract_frames(self, video_path):\n",
    "        \"\"\"Extract frames from video\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            # Return shape: [num_frames, 3, H, W]\n",
    "            return torch.randn(self.num_frames, 3, self.frame_size, self.frame_size)\n",
    "        \n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "                frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        while len(frames) < self.num_frames:\n",
    "            if frames:\n",
    "                frames.append(frames[-1].clone())\n",
    "            else:\n",
    "                frames.append(torch.zeros(3, self.frame_size, self.frame_size))\n",
    "        \n",
    "        # Stack to [num_frames, 3, H, W]\n",
    "        frames = torch.stack(frames[:self.num_frames])\n",
    "        return frames\n",
    "\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 2. DATA AUGMENTATION\n",
    "# ========================\n",
    "\n",
    "class VideoTransform:\n",
    "    \"\"\"Video augmentation\"\"\"\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"frames: [T, C, H, W]\"\"\"\n",
    "        if self.mode == 'train':\n",
    "            # Random horizontal flip\n",
    "            if torch.rand(1) > 0.5:\n",
    "                frames = torch.flip(frames, dims=[3])\n",
    "            \n",
    "            # Brightness\n",
    "            brightness = 0.8 + torch.rand(1) * 0.4\n",
    "            frames = torch.clamp(frames * brightness, 0, 1)\n",
    "        \n",
    "        # Normalize (ImageNet stats)\n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "        frames = (frames - mean) / std\n",
    "        \n",
    "        return frames\n",
    "\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 3. TIMESFORMER MODEL (HUGGING FACE)\n",
    "# ========================\n",
    "\n",
    "class TimeSformerForEmotionRecognition(nn.Module):\n",
    "    \"\"\"\n",
    "    TimeSformer model using Hugging Face Transformers\n",
    "    - Uses pretrained weights from Facebook Research\n",
    "    - Implements divided space-time attention\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, num_frames=8, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"\\nüèóÔ∏è  Building TimeSformer model...\")\n",
    "        \n",
    "        if pretrained:\n",
    "            print(\"   Loading pretrained TimeSformer from Facebook Research...\")\n",
    "            # Load pretrained TimeSformer (base model)\n",
    "            self.timesformer = TimesformerModel.from_pretrained(\n",
    "                \"facebook/timesformer-base-finetuned-k400\",\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "            print(\"   ‚úì Pretrained weights loaded from Kinetics-400\")\n",
    "        else:\n",
    "            print(\"   Initializing TimeSformer from scratch...\")\n",
    "            config = TimesformerConfig(\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                num_channels=3,\n",
    "                num_frames=num_frames,\n",
    "                hidden_size=768,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072,\n",
    "                attention_type=\"divided_space_time\"\n",
    "            )\n",
    "            self.timesformer = TimesformerModel(config)\n",
    "            print(\"   ‚úì Model initialized from scratch\")\n",
    "        \n",
    "        # Get hidden size\n",
    "        hidden_size = self.timesformer.config.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"   Architecture:\")\n",
    "        print(f\"   - Attention Type: Divided Space-Time\")\n",
    "        print(f\"   - Hidden Size: {hidden_size}\")\n",
    "        print(f\"   - Transformer Layers: {self.timesformer.config.num_hidden_layers}\")\n",
    "        print(f\"   - Attention Heads: {self.timesformer.config.num_attention_heads}\")\n",
    "        print(f\"   - Output Classes: {num_classes}\")\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pixel_values: [batch_size, num_frames, num_channels, height, width]\n",
    "        \"\"\"\n",
    "        # HuggingFace TimeSformer expects: [batch_size, num_frames, num_channels, height, width]\n",
    "        # Our dataloader gives: [batch_size, num_frames, num_channels, height, width]\n",
    "        # So we DON'T need to permute - it's already correct!\n",
    "        \n",
    "        # Get TimeSformer outputs\n",
    "        outputs = self.timesformer(pixel_values)\n",
    "        \n",
    "        # Use the [CLS] token representation\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        cls_token = sequence_output[:, 0]  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 4. TRAINING FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for videos, labels in pbar:\n",
    "        videos = videos.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(videos)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(loader, desc='Validating'):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    return total_loss / len(loader), acc, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 5. MAIN SCRIPT\n",
    "# ========================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"TIMESFORMER VIDEO EMOTION RECOGNITION\")\n",
    "    print(\"Using Hugging Face Transformers + Facebook Research Pretrained Model\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    config = {\n",
    "        'num_frames': 8,\n",
    "        'frame_size': 224,\n",
    "        'batch_size': 2,\n",
    "        'num_epochs': 5,\n",
    "        'learning_rate': 3e-5,\n",
    "        'weight_decay': 0.01,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'use_pretrained': True,  # Use pretrained weights\n",
    "        'train_split': 0.6,\n",
    "        'val_split': 0.2,\n",
    "        'test_split': 0.2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\nüìÅ Loading dataset...\")\n",
    "    train_dataset = SyntheticEmotionsDataset(\n",
    "        split='train',\n",
    "        num_frames=config['num_frames'],\n",
    "        frame_size=config['frame_size'],\n",
    "        transform=VideoTransform(mode='train')\n",
    "    )\n",
    "    \n",
    "    val_dataset = SyntheticEmotionsDataset(\n",
    "        split='train',\n",
    "        num_frames=config['num_frames'],\n",
    "        frame_size=config['frame_size'],\n",
    "        transform=VideoTransform(mode='val')\n",
    "    )\n",
    "    \n",
    "    # Split data\n",
    "    dataset_size = len(train_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_size = int(config['train_split'] * dataset_size)\n",
    "    val_size = int(config['val_split'] * dataset_size)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size+val_size]\n",
    "    test_indices = indices[train_size+val_size:]\n",
    "    \n",
    "    print(f\"\\nüìä Splits: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "    \n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(val_dataset, val_indices)\n",
    "    test_subset = Subset(val_dataset, test_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=config['batch_size'], \n",
    "                             shuffle=True, num_workers=0, drop_last=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=config['batch_size'], \n",
    "                           shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_subset, batch_size=config['batch_size'], \n",
    "                            shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Create model\n",
    "    model = TimeSformerForEmotionRecognition(\n",
    "        num_classes=len(train_dataset.emotions),\n",
    "        num_frames=config['num_frames'],\n",
    "        pretrained=config['use_pretrained']\n",
    "    ).to(config['device'])\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6\n",
    "    print(f\"\\nüìà Parameters: {total_params:.2f}M total, {trainable_params:.2f}M trainable\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'], \n",
    "                           weight_decay=config['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['num_epochs'])\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ STARTING TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nüìÖ Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config['device']\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_f1, _, _ = validate(\n",
    "            model, val_loader, criterion, config['device']\n",
    "        )\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
    "        print(f\"Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_timesformer_hf.pth')\n",
    "            print(f\"‚úÖ New best model saved!\")\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_timesformer_hf.pth'))\n",
    "    test_loss, test_acc, test_f1, test_preds, test_labels = validate(\n",
    "        model, test_loader, criterion, config['device']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"  Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7195832,
     "sourceId": 11495578,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31240,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
