{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26e0142",
   "metadata": {},
   "source": [
    "Mohammed elidrissi laoukili\n",
    "* subjet  : video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1d62265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "WARNING: Training on CPU will be very slow!\n",
      "============================================================\n",
      "\n",
      "Loading dataset...\n",
      "Loading synthetic-emotions dataset (split: train)...\n",
      "Loaded 100 videos\n",
      "Emotions found: ['Anger', 'Confusion', 'Disgust', 'Fear', 'Happiness and Joy', 'Love and Affection', 'Mixed Emotions', 'Neutral/Everyday', 'Sadness', 'Surprise']\n",
      "Emotion distribution: {'Happiness and Joy': 10, 'Anger': 10, 'Sadness': 10, 'Fear': 10, 'Surprise': 10, 'Disgust': 10, 'Love and Affection': 10, 'Confusion': 10, 'Neutral/Everyday': 10, 'Mixed Emotions': 10}\n",
      "Number of emotion classes: 10\n",
      "\n",
      "Dataset splits:\n",
      "  Train: 70 videos\n",
      "  Val: 15 videos\n",
      "  Test: 15 videos\n",
      "\n",
      "Creating model...\n",
      "Model parameters: 14.57M\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Epoch 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [02:58<00:00,  9.89s/it, loss=2.9118, acc=7.14%]\n",
      "Validating: 100%|██████████| 4/4 [00:08<00:00,  2.10s/it, loss=1.9317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary:\n",
      "  Train Loss: 2.7621, Train Acc: 0.0714\n",
      "  Val Loss: 2.4241, Val Acc: 0.1333, Val F1: 0.0314\n",
      "  ✓ Saved best model with val_acc: 0.1333\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [02:31<00:00,  8.40s/it, loss=2.3811, acc=10.00%]\n",
      "Validating: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it, loss=2.3834]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary:\n",
      "  Train Loss: 2.4733, Train Acc: 0.1000\n",
      "  Val Loss: 2.3749, Val Acc: 0.0000, Val F1: 0.0000\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [02:12<00:00,  7.35s/it, loss=2.1770, acc=10.00%]\n",
      "Validating: 100%|██████████| 4/4 [00:08<00:00,  2.15s/it, loss=2.1806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary:\n",
      "  Train Loss: 2.3585, Train Acc: 0.1000\n",
      "  Val Loss: 2.3619, Val Acc: 0.1333, Val F1: 0.0314\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [02:28<00:00,  8.22s/it, loss=2.5264, acc=7.14%]\n",
      "Validating: 100%|██████████| 4/4 [00:09<00:00,  2.45s/it, loss=2.2900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Summary:\n",
      "  Train Loss: 2.3429, Train Acc: 0.0714\n",
      "  Val Loss: 2.3919, Val Acc: 0.0667, Val F1: 0.0083\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 18/18 [02:28<00:00,  8.26s/it, loss=2.3123, acc=11.43%]\n",
      "Validating: 100%|██████████| 4/4 [00:08<00:00,  2.05s/it, loss=2.2934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Summary:\n",
      "  Train Loss: 2.3037, Train Acc: 0.1143\n",
      "  Val Loss: 2.3757, Val Acc: 0.1333, Val F1: 0.0314\n",
      "\n",
      "Training curves saved to 'training_curves.png'\n",
      "\n",
      "============================================================\n",
      "FINAL TEST EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 4/4 [00:09<00:00,  2.37s/it, loss=2.9783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 2.5039\n",
      "Test Accuracy: 0.0000\n",
      "Test F1-Score: 0.0000\n",
      "\n",
      "Classification Report:\n",
      "Note: Only 8 out of 10 classes present in test set\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Anger     0.0000    0.0000    0.0000       1.0\n",
      "         Confusion     0.0000    0.0000    0.0000       2.0\n",
      "           Disgust     0.0000    0.0000    0.0000       2.0\n",
      " Happiness and Joy     0.0000    0.0000    0.0000       1.0\n",
      "Love and Affection     0.0000    0.0000    0.0000       1.0\n",
      "    Mixed Emotions     0.0000    0.0000    0.0000       2.0\n",
      "  Neutral/Everyday     0.0000    0.0000    0.0000       3.0\n",
      "           Sadness     0.0000    0.0000    0.0000       3.0\n",
      "\n",
      "         micro avg     0.0000    0.0000    0.0000      15.0\n",
      "         macro avg     0.0000    0.0000    0.0000      15.0\n",
      "      weighted avg     0.0000    0.0000    0.0000      15.0\n",
      "\n",
      "Confusion matrix saved to 'confusion_matrix.png'\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "============================================================\n",
      "Best validation accuracy: 0.1333\n",
      "Final test accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 1. HUGGING FACE DATASET CLASS\n",
    "# ========================\n",
    "\n",
    "class SyntheticEmotionsDataset(Dataset):\n",
    "    \"\"\"Dataset wrapper for aadityaubhat/synthetic-emotions from Hugging Face\"\"\"\n",
    "    def __init__(self, split='train', num_frames=16, frame_size=224, transform=None):\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        \n",
    "        print(f\"Loading synthetic-emotions dataset (split: {split})...\")\n",
    "        \n",
    "        try:\n",
    "            dataset = load_dataset(\"aadityaubhat/synthetic-emotions\", split=split)\n",
    "            if \"video\" in dataset.features:\n",
    "                dataset = dataset.cast_column(\"video\", dataset.features[\"video\"])\n",
    "            self.dataset = dataset\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        self.samples = []\n",
    "        self.emotion_counts = {}\n",
    "        \n",
    "        labels = None\n",
    "        if 'label' in dataset.column_names:\n",
    "            labels = dataset['label']\n",
    "        elif 'emotion' in dataset.column_names:\n",
    "            labels = dataset['emotion']\n",
    "        else:\n",
    "            raise ValueError(\"Dataset has no label/emotion column\")\n",
    "\n",
    "        for idx, emotion in enumerate(labels):\n",
    "            self.samples.append((idx, emotion))\n",
    "            self.emotion_counts[emotion] = self.emotion_counts.get(emotion, 0) + 1\n",
    "        \n",
    "        self.emotions = sorted(list(set([s[1] for s in self.samples])))\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.emotions)}\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} videos\")\n",
    "        print(f\"Emotions found: {self.emotions}\")\n",
    "        print(f\"Emotion distribution: {self.emotion_counts}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx, emotion = self.samples[idx]\n",
    "\n",
    "        try:\n",
    "            item = self.dataset[sample_idx]\n",
    "            video_data = item['video']\n",
    "            frames = self.load_video_from_hf(video_data)\n",
    "        except Exception as e:\n",
    "            frames = torch.zeros(3, self.num_frames, self.frame_size, self.frame_size)\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        label = self.emotion_to_idx[emotion]\n",
    "        return frames, label\n",
    "    \n",
    "    def load_video_from_hf(self, video_data):\n",
    "        \"\"\"Load video from Hugging Face dataset item\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n",
    "            if isinstance(video_data, dict) and 'bytes' in video_data:\n",
    "                tmp.write(video_data['bytes'])\n",
    "            elif isinstance(video_data, bytes):\n",
    "                tmp.write(video_data)\n",
    "            else:\n",
    "                video_path = video_data.get('path', video_data)\n",
    "                if os.path.exists(video_path):\n",
    "                    with open(video_path, 'rb') as f:\n",
    "                        tmp.write(f.read())\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        frames = self.load_video_from_path(tmp_path)\n",
    "        \n",
    "        try:\n",
    "            os.unlink(tmp_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def load_video_from_path(self, video_path):\n",
    "        \"\"\"Load video and sample frames uniformly\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total_frames == 0:\n",
    "            cap.release()\n",
    "            return torch.randn(3, self.num_frames, self.frame_size, self.frame_size)\n",
    "        \n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        frames = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.frame_size, self.frame_size))\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        while len(frames) < self.num_frames:\n",
    "            if frames:\n",
    "                frames.append(frames[-1].copy())\n",
    "            else:\n",
    "                frames.append(np.zeros((self.frame_size, self.frame_size, 3), dtype=np.uint8))\n",
    "        \n",
    "        frames = np.stack(frames[:self.num_frames])\n",
    "        frames = torch.from_numpy(frames).permute(3, 0, 1, 2).float()\n",
    "        frames = frames / 255.0\n",
    "        \n",
    "        return frames\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 2. DATA AUGMENTATION\n",
    "# ========================\n",
    "\n",
    "class VideoAugmentation:\n",
    "    \"\"\"Data augmentation for video frames\"\"\"\n",
    "    def __init__(self, mode='train'):\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __call__(self, frames):\n",
    "        \"\"\"frames: tensor of shape [C, T, H, W]\"\"\"\n",
    "        if self.mode == 'train':\n",
    "            if torch.rand(1) > 0.5:\n",
    "                frames = torch.flip(frames, dims=[3])\n",
    "            \n",
    "            brightness_factor = 0.8 + torch.rand(1) * 0.4\n",
    "            frames = torch.clamp(frames * brightness_factor, 0, 1)\n",
    "            \n",
    "            c, t, h, w = frames.shape\n",
    "            crop_size = int(h * (0.8 + torch.rand(1) * 0.2))\n",
    "            top = torch.randint(0, h - crop_size + 1, (1,)).item()\n",
    "            left = torch.randint(0, w - crop_size + 1, (1,)).item()\n",
    "            frames = frames[:, :, top:top+crop_size, left:left+crop_size]\n",
    "            frames = torch.nn.functional.interpolate(\n",
    "                frames.permute(1, 0, 2, 3),\n",
    "                size=(h, w),\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            ).permute(1, 0, 2, 3)\n",
    "        \n",
    "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1)\n",
    "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1)\n",
    "        frames = (frames - mean) / std\n",
    "        \n",
    "        return frames\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 3. TIMESFORMER MODEL (FIXED - NO IN-PLACE OPERATIONS)\n",
    "# ========================\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Convert video to patch embeddings\"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(B * T, C, H, W)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = x.reshape(B, T, self.num_patches, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DividedSpaceTimeAttention(nn.Module):\n",
    "    \"\"\"Divided Space-Time Attention (FIXED - No in-place operations)\"\"\"\n",
    "    def __init__(self, dim, num_heads=12, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        \n",
    "        self.qkv_spatial = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop_spatial = nn.Dropout(attn_drop)\n",
    "        self.proj_spatial = nn.Linear(dim, dim)\n",
    "        self.proj_drop_spatial = nn.Dropout(proj_drop)\n",
    "        \n",
    "        self.qkv_temporal = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop_temporal = nn.Dropout(attn_drop)\n",
    "        self.proj_temporal = nn.Linear(dim, dim)\n",
    "        self.proj_drop_temporal = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, N, D = x.shape\n",
    "        \n",
    "        # Temporal attention on CLS tokens\n",
    "        cls_tokens = x[:, :, 0, :].clone()  # Clone to avoid in-place issues\n",
    "        qkv = self.qkv_temporal(cls_tokens).reshape(B, T, 3, self.num_heads, D // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop_temporal(attn)\n",
    "        \n",
    "        cls_tokens_out = (attn @ v).transpose(1, 2).reshape(B, T, D)\n",
    "        cls_tokens_out = self.proj_temporal(cls_tokens_out)\n",
    "        cls_tokens_out = self.proj_drop_temporal(cls_tokens_out)\n",
    "        \n",
    "        # FIXED: Create new tensor instead of in-place operation\n",
    "        x_new = x.clone()\n",
    "        x_new[:, :, 0, :] = x[:, :, 0, :] + cls_tokens_out\n",
    "        \n",
    "        # Spatial attention\n",
    "        x_spatial = x_new.reshape(B * T, N, D)\n",
    "        qkv = self.qkv_spatial(x_spatial).reshape(B * T, N, 3, self.num_heads, D // self.num_heads)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop_spatial(attn)\n",
    "        \n",
    "        x_spatial = (attn @ v).transpose(1, 2).reshape(B * T, N, D)\n",
    "        x_spatial = self.proj_spatial(x_spatial)\n",
    "        x_spatial = self.proj_drop_spatial(x_spatial)\n",
    "        \n",
    "        # FIXED: Add residual without in-place operation\n",
    "        x_out = x_new.reshape(B * T, N, D) + x_spatial\n",
    "        x_out = x_out.reshape(B, T, N, D)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with divided space-time attention\"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = DividedSpaceTimeAttention(dim, num_heads, qkv_bias, attn_drop, drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        \n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimeSformer(nn.Module):\n",
    "    \"\"\"TimeSformer for video emotion recognition\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=7,\n",
    "        embed_dim=384,\n",
    "        depth=6,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.,\n",
    "        qkv_bias=True,\n",
    "        drop_rate=0.1,\n",
    "        attn_drop_rate=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, T, H, W = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, T, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=2)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = x.mean(dim=[1, 2])\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 4. TRAINING FUNCTIONS\n",
    "# ========================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for videos, labels in pbar:\n",
    "        try:\n",
    "            videos = videos.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in training batch: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validating')\n",
    "        for videos, labels in pbar:\n",
    "            try:\n",
    "                videos = videos.to(device, non_blocking=True)\n",
    "                labels = labels.to(device, non_blocking=True)\n",
    "                \n",
    "                outputs = model(videos)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError in validation batch: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return total_loss / len(loader), accuracy, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    print(\"Confusion matrix saved to 'confusion_matrix.png'\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# ========================\n",
    "# 5. MAIN TRAINING SCRIPT\n",
    "# ========================\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'num_frames': 8,\n",
    "        'frame_size': 224,\n",
    "        'patch_size': 16,\n",
    "        'embed_dim': 384,\n",
    "        'depth': 6,\n",
    "        'num_heads': 6,\n",
    "        'batch_size': 4,\n",
    "        'num_epochs': 5,\n",
    "        'learning_rate': 3e-4,\n",
    "        'weight_decay': 0.05,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'train_split': 0.7,\n",
    "        'val_split': 0.15,\n",
    "        'test_split': 0.15\n",
    "    }\n",
    "    \n",
    "    print(f\"Using device: {config['device']}\")\n",
    "    if config['device'] == 'cpu':\n",
    "        print(\"WARNING: Training on CPU will be very slow!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\\nLoading dataset...\")\n",
    "    full_dataset = SyntheticEmotionsDataset(\n",
    "        split='train',\n",
    "        num_frames=config['num_frames'],\n",
    "        frame_size=config['frame_size'],\n",
    "        transform=VideoAugmentation(mode='train')\n",
    "    )\n",
    "    \n",
    "    config['num_classes'] = len(full_dataset.emotions)\n",
    "    print(f\"Number of emotion classes: {config['num_classes']}\")\n",
    "    \n",
    "    dataset_size = len(full_dataset)\n",
    "    indices = list(range(dataset_size))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_size = int(config['train_split'] * dataset_size)\n",
    "    val_size = int(config['val_split'] * dataset_size)\n",
    "    \n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size+val_size]\n",
    "    test_indices = indices[train_size+val_size:]\n",
    "    \n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"  Train: {len(train_indices)} videos\")\n",
    "    print(f\"  Val: {len(val_indices)} videos\")\n",
    "    print(f\"  Test: {len(test_indices)} videos\")\n",
    "    \n",
    "    from torch.utils.data import Subset\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    print(\"\\nCreating model...\")\n",
    "    model = TimeSformer(\n",
    "        img_size=config['frame_size'],\n",
    "        patch_size=config['patch_size'],\n",
    "        num_classes=config['num_classes'],\n",
    "        embed_dim=config['embed_dim'],\n",
    "        depth=config['depth'],\n",
    "        num_heads=config['num_heads']\n",
    "    ).to(config['device'])\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=config['num_epochs']\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(config['num_epochs']):\n",
    "            print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(\n",
    "                model, train_loader, criterion, optimizer, config['device']\n",
    "            )\n",
    "            \n",
    "            val_loss, val_acc, val_f1, _, _ = validate(\n",
    "                model, val_loader, criterion, config['device']\n",
    "            )\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(f\"  ✓ Saved best model with val_acc: {val_acc:.4f}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nTraining interrupted by user!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nError during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "    \n",
    "    if len(train_losses) > 0:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "        plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_accs, label='Train Acc', marker='o')\n",
    "        plt.plot(val_accs, label='Val Acc', marker='s')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('training_curves.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"\\nTraining curves saved to 'training_curves.png'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load('best_model.pth'))\n",
    "        test_loss, test_acc, test_f1, test_preds, test_labels = validate(\n",
    "            model, test_loader, criterion, config['device']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test F1-Score: {test_f1:.4f}\")\n",
    "        \n",
    "        # Get unique classes in test set\n",
    "        unique_test_classes = sorted(list(set(test_labels)))\n",
    "        test_class_names = [full_dataset.emotions[i] for i in unique_test_classes]\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(f\"Note: Only {len(unique_test_classes)} out of {len(full_dataset.emotions)} classes present in test set\")\n",
    "        print(classification_report(\n",
    "            test_labels, \n",
    "            test_preds,\n",
    "            labels=unique_test_classes,\n",
    "            target_names=test_class_names,\n",
    "            digits=4,\n",
    "            zero_division=0\n",
    "        ))\n",
    "        \n",
    "        plot_confusion_matrix(\n",
    "            test_labels,\n",
    "            test_preds,\n",
    "            test_class_names\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        print(f\"Final test accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nNo saved model found. Training may not have completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during testing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
