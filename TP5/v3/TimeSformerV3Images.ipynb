{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mohammed elidrissi laoukili\n",
    "* subjet  : video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-01-02T16:45:47.939178Z",
     "iopub.status.busy": "2026-01-02T16:45:47.938546Z",
     "iopub.status.idle": "2026-01-02T16:45:47.943368Z",
     "shell.execute_reply": "2026-01-02T16:45:47.942621Z",
     "shell.execute_reply.started": "2026-01-02T16:45:47.939148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:45:47.947526Z",
     "iopub.status.busy": "2026-01-02T16:45:47.947061Z",
     "iopub.status.idle": "2026-01-02T16:46:16.494153Z",
     "shell.execute_reply": "2026-01-02T16:46:16.493456Z",
     "shell.execute_reply.started": "2026-01-02T16:45:47.947503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 16:46:01.411562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1767372361.659518      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1767372361.727087      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1767372362.346701      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767372362.346744      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767372362.346747      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1767372362.346750      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TimeSformer for Facial Expression Recognition on Kaggle\n",
    "Adapted for image-based dataset with YOLO format\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from transformers import TimesformerModel, TimesformerConfig\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:46:16.495935Z",
     "iopub.status.busy": "2026-01-02T16:46:16.495365Z",
     "iopub.status.idle": "2026-01-02T16:46:16.508314Z",
     "shell.execute_reply": "2026-01-02T16:46:16.507615Z",
     "shell.execute_reply.started": "2026-01-02T16:46:16.495908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1. DATASET CLASS\n",
    "# ========================\n",
    "\n",
    "class FacialExpressionDataset(Dataset):\n",
    "    \"\"\"Dataset for facial expressions with temporal augmentation\"\"\"\n",
    "    def __init__(self, base_dir, split='train', num_frames=8, frame_size=224, transform=None):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.split = split\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Emotion mapping\n",
    "        self.emotion_map = {\n",
    "            0: 'angry', 1: 'contempt', 2: 'disgust', 3: 'fear', \n",
    "            4: 'happy', 5: 'natural', 6: 'sad', 7: 'sleepy', 8: 'surprised'\n",
    "        }\n",
    "        self.idx_to_emotion = {v: k for k, v in self.emotion_map.items()}\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.samples = self._load_samples()\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} images from {split} split\")\n",
    "        self._print_distribution()\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"Load all image paths and extract labels from YOLO format\"\"\"\n",
    "        samples = []\n",
    "        \n",
    "        images_dir = self.base_dir / self.split / 'images'\n",
    "        labels_dir = self.base_dir / self.split / 'labels'\n",
    "        \n",
    "        if not images_dir.exists():\n",
    "            raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n",
    "        \n",
    "        for img_path in sorted(images_dir.glob('*.[jp][pn]g')):\n",
    "            # Get corresponding label file\n",
    "            label_path = labels_dir / f\"{img_path.stem}.txt\"\n",
    "            \n",
    "            if label_path.exists():\n",
    "                # Read YOLO label (first number is class)\n",
    "                with open(label_path, 'r') as f:\n",
    "                    line = f.readline().strip()\n",
    "                    if line:\n",
    "                        class_id = int(line.split()[0])\n",
    "                        samples.append((str(img_path), class_id))\n",
    "            else:\n",
    "                # If no label, try to infer from filename or skip\n",
    "                print(f\"Warning: No label found for {img_path.name}\")\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def _print_distribution(self):\n",
    "        \"\"\"Print class distribution\"\"\"\n",
    "        label_counts = {}\n",
    "        for _, label in self.samples:\n",
    "            emotion = self.emotion_map.get(label, 'unknown')\n",
    "            label_counts[emotion] = label_counts.get(emotion, 0) + 1\n",
    "        \n",
    "        print(f\"\\nClass distribution ({self.split}):\")\n",
    "        for emotion, count in sorted(label_counts.items()):\n",
    "            print(f\"  {emotion}: {count}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Create temporal sequence by augmenting the same image\n",
    "            frames = self._create_temporal_sequence(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return dummy data if loading fails\n",
    "            frames = torch.zeros(self.num_frames, 3, self.frame_size, self.frame_size)\n",
    "        \n",
    "        return frames, label\n",
    "    \n",
    "    def _create_temporal_sequence(self, image):\n",
    "        \"\"\"\n",
    "        Create a temporal sequence from a single image\n",
    "        Strategy: Apply different augmentations to create pseudo-temporal data\n",
    "        \"\"\"\n",
    "        # Resize base image once\n",
    "        image = cv2.resize(image, (self.frame_size, self.frame_size))\n",
    "        \n",
    "        frames = []\n",
    "        for i in range(self.num_frames):\n",
    "            if self.transform:\n",
    "                # Apply augmentation (includes normalization)\n",
    "                augmented = self.transform(image=image.copy())\n",
    "                frame = augmented['image']\n",
    "            else:\n",
    "                # Just convert to tensor and normalize\n",
    "                frame = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "                mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "                frame = (frame - mean) / std\n",
    "            \n",
    "            frames.append(frame)\n",
    "        \n",
    "        # Stack: [num_frames, 3, H, W]\n",
    "        frames = torch.stack(frames)\n",
    "        \n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:46:16.509608Z",
     "iopub.status.busy": "2026-01-02T16:46:16.509309Z",
     "iopub.status.idle": "2026-01-02T16:46:16.531558Z",
     "shell.execute_reply": "2026-01-02T16:46:16.530815Z",
     "shell.execute_reply.started": "2026-01-02T16:46:16.509575Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2. AUGMENTATION\n",
    "# ========================\n",
    "\n",
    "def get_transforms(mode='train', img_size=224):\n",
    "    \"\"\"Get augmentation pipeline - returns numpy arrays for efficiency\"\"\"\n",
    "    if mode == 'train':\n",
    "        return A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(limit=15, p=0.5),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "            A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "            A.GaussianBlur(blur_limit=(3, 5), p=0.3),\n",
    "            A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.3),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:46:16.533120Z",
     "iopub.status.busy": "2026-01-02T16:46:16.532866Z",
     "iopub.status.idle": "2026-01-02T16:46:16.545297Z",
     "shell.execute_reply": "2026-01-02T16:46:16.544657Z",
     "shell.execute_reply.started": "2026-01-02T16:46:16.533099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 3. MODEL\n",
    "# ========================\n",
    "\n",
    "class TimeSformerForFacialExpression(nn.Module):\n",
    "    \"\"\"TimeSformer model for facial expression recognition\"\"\"\n",
    "    def __init__(self, num_classes=9, num_frames=8, pretrained=True, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"\\nüèóÔ∏è  Building TimeSformer model...\")\n",
    "        \n",
    "        if pretrained:\n",
    "            print(\"   Loading pretrained TimeSformer...\")\n",
    "            self.timesformer = TimesformerModel.from_pretrained(\n",
    "                \"facebook/timesformer-base-finetuned-k400\",\n",
    "                ignore_mismatched_sizes=True\n",
    "            )\n",
    "            print(\"   ‚úì Pretrained weights loaded\")\n",
    "        else:\n",
    "            config = TimesformerConfig(\n",
    "                image_size=224,\n",
    "                patch_size=16,\n",
    "                num_channels=3,\n",
    "                num_frames=num_frames,\n",
    "                hidden_size=768,\n",
    "                num_hidden_layers=12,\n",
    "                num_attention_heads=12,\n",
    "                intermediate_size=3072,\n",
    "                attention_type=\"divided_space_time\"\n",
    "            )\n",
    "            self.timesformer = TimesformerModel(config)\n",
    "        \n",
    "        # Optionally freeze backbone\n",
    "        if freeze_backbone:\n",
    "            print(\"   Freezing TimeSformer backbone...\")\n",
    "            for param in self.timesformer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        hidden_size = self.timesformer.config.hidden_size\n",
    "        \n",
    "        # Enhanced classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        print(f\"   Output Classes: {num_classes}\")\n",
    "        print(f\"   Backbone Frozen: {freeze_backbone}\")\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # pixel_values: [batch_size, num_frames, channels, height, width]\n",
    "        outputs = self.timesformer(pixel_values)\n",
    "        cls_token = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(cls_token)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:46:16.546431Z",
     "iopub.status.busy": "2026-01-02T16:46:16.546152Z",
     "iopub.status.idle": "2026-01-02T16:46:16.558919Z",
     "shell.execute_reply": "2026-01-02T16:46:16.558257Z",
     "shell.execute_reply.started": "2026-01-02T16:46:16.546401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 4. TRAINING\n",
    "# ========================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device, scaler=None, accumulation_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch_idx, (frames, labels) in enumerate(pbar):\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Mixed precision training with gradient accumulation\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / accumulation_steps  # Scale loss\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "            \n",
    "            if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item() * accumulation_steps:.4f}',\n",
    "            'acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for frames, labels in tqdm(loader, desc='Validating'):\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    return total_loss / len(loader), acc, f1, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:46:16.560030Z",
     "iopub.status.busy": "2026-01-02T16:46:16.559793Z",
     "iopub.status.idle": "2026-01-02T16:46:16.573114Z",
     "shell.execute_reply": "2026-01-02T16:46:16.572537Z",
     "shell.execute_reply.started": "2026-01-02T16:46:16.560010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, emotion_map, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Plot and save confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=emotion_map.values(),\n",
    "                yticklabels=emotion_map.values())\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Confusion matrix saved to {save_path}\")\n",
    "\n",
    "\n",
    "def plot_training_history(history, save_path='training_history.png'):\n",
    "    \"\"\"Plot and save training history\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[1].plot([a*100 for a in history['train_acc']], label='Train Acc', marker='o')\n",
    "    axes[1].plot([a*100 for a in history['val_acc']], label='Val Acc', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Training and Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Training history saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-02T16:46:16.574215Z",
     "iopub.status.busy": "2026-01-02T16:46:16.573946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TIMESFORMER FACIAL EXPRESSION RECOGNITION\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  base_dir: /kaggle/input/8-facial-expressions-for-yolo/9 Facial Expressions you need\n",
      "  num_frames: 8\n",
      "  frame_size: 224\n",
      "  batch_size: 4\n",
      "  num_epochs: 3\n",
      "  learning_rate: 5e-05\n",
      "  weight_decay: 0.01\n",
      "  device: cuda\n",
      "  use_pretrained: True\n",
      "  freeze_backbone: True\n",
      "  num_workers: 2\n",
      "  mixed_precision: True\n",
      "  accumulation_steps: 4\n",
      "\n",
      "üìÅ Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/2997857931.py:12: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
      "/tmp/ipykernel_55/2997857931.py:14: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=1, max_height=32, max_width=32, p=0.3),\n"
     ]
    }
   ],
   "source": [
    "# ========================\n",
    "# 5. MAIN\n",
    "# ========================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*80)\n",
    "    print(\"TIMESFORMER FACIAL EXPRESSION RECOGNITION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'base_dir': '/kaggle/input/8-facial-expressions-for-yolo/9 Facial Expressions you need',\n",
    "        'num_frames': 8,\n",
    "        'frame_size': 224,\n",
    "        'batch_size': 4,  # Reduced for memory efficiency with TimeSformer\n",
    "        'num_epochs': 3,\n",
    "        'learning_rate': 5e-5,  # Lower initial LR for stability\n",
    "        'weight_decay': 0.01,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'use_pretrained': True,\n",
    "        'freeze_backbone': True,  # Freeze for first epochs\n",
    "        'num_workers': 2,\n",
    "        'mixed_precision': True,\n",
    "        'accumulation_steps': 4,  # Gradient accumulation for effective batch size\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    for k, v in config.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"\\nüìÅ Loading datasets...\")\n",
    "    train_dataset = FacialExpressionDataset(\n",
    "        config['base_dir'], \n",
    "        split='train',\n",
    "        num_frames=config['num_frames'],\n",
    "        frame_size=config['frame_size'],\n",
    "        transform=get_transforms('train', config['frame_size'])\n",
    "    )\n",
    "    \n",
    "    val_dataset = FacialExpressionDataset(\n",
    "        config['base_dir'],\n",
    "        split='valid',\n",
    "        num_frames=config['num_frames'],\n",
    "        frame_size=config['frame_size'],\n",
    "        transform=get_transforms('val', config['frame_size'])\n",
    "    )\n",
    "    \n",
    "    test_dataset = FacialExpressionDataset(\n",
    "        config['base_dir'],\n",
    "        split='test',\n",
    "        num_frames=config['num_frames'],\n",
    "        frame_size=config['frame_size'],\n",
    "        transform=get_transforms('val', config['frame_size'])\n",
    "    )\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True, \n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Model\n",
    "    model = TimeSformerForFacialExpression(\n",
    "        num_classes=9,\n",
    "        num_frames=config['num_frames'],\n",
    "        pretrained=config['use_pretrained'],\n",
    "        freeze_backbone=config['freeze_backbone']\n",
    "    ).to(config['device'])\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters()) / 1e6\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6\n",
    "    print(f\"\\nüìà Parameters: {total_params:.2f}M total, {trainable_params:.2f}M trainable\")\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=config['num_epochs']\n",
    "    )\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler() if config['mixed_precision'] and config['device'] == 'cuda' else None\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    best_val_f1 = 0\n",
    "    patience = 7\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [], 'val_f1': []}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üöÄ STARTING TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nüìÖ Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        \n",
    "        # Unfreeze backbone after some epochs\n",
    "        if config['freeze_backbone'] and epoch == 5:\n",
    "            print(\"   üîì Unfreezing backbone...\")\n",
    "            for param in model.timesformer.parameters():\n",
    "                param.requires_grad = True\n",
    "            # Rebuild optimizer with all parameters and lower LR\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'] / 10,\n",
    "                weight_decay=config['weight_decay']\n",
    "            )\n",
    "            # Reset scheduler for remaining epochs\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, \n",
    "                T_max=config['num_epochs'] - epoch\n",
    "            )\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config['device'], \n",
    "            scaler, config['accumulation_steps']\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_f1, _, _ = validate(\n",
    "            model, val_loader, criterion, config['device']\n",
    "        )\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        \n",
    "        print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc*100:.2f}%\")\n",
    "        print(f\"Val:   Loss={val_loss:.4f}, Acc={val_acc*100:.2f}%, F1={val_f1:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_f1': val_f1,\n",
    "                'config': config,\n",
    "            }, 'best_timesformer_facial.pth')\n",
    "            print(f\"‚úÖ New best model saved! Val Acc: {val_acc*100:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\n‚ö†Ô∏è  Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Test evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üéØ FINAL TEST EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    checkpoint = torch.load('best_timesformer_facial.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_loss, test_acc, test_f1, test_preds, test_labels = validate(\n",
    "        model, test_loader, criterion, config['device']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìä Test Results:\")\n",
    "    print(f\"  Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(test_labels, test_preds, train_dataset.emotion_map)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Print per-class metrics\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(\"\\nüìã Per-Class Metrics:\")\n",
    "    print(classification_report(test_labels, test_preds, \n",
    "                                target_names=list(train_dataset.emotion_map.values()),\n",
    "                                digits=4))\n",
    "    \n",
    "    print(\"\\n‚úÖ Training complete!\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc*100:.2f}%\")\n",
    "    print(f\"Best validation F1-score: {best_val_f1:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7195832,
     "sourceId": 11495578,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
